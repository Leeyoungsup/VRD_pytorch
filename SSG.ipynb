{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import h5py\n",
    "import json\n",
    "import torch\n",
    "from imageio import imread\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from random import seed, choice, sample\n",
    "from torchvision.transforms import ToTensor\n",
    "import pickle\n",
    "import nltk\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "# resize ->numpy.array(Image.fromarray(arr).resize())\n",
    "import torch.utils.data as data\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "batch_size=4\n",
    "img_size=256\n",
    "tf = ToTensor()\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "캡션을 이용해 Vocabulary 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/gil/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "vocab_path = \"./vocab.pkl\" # 단어 사전 결과 파일\n",
    "word_threshold = 4 # 최소 단어 등장 횟수\n",
    "counter = Counter()\n",
    "crop_size = 224 # 랜덤하게 잘라낼 이미지 크기\n",
    "train_transform = transforms.Compose([ \n",
    "    transforms.RandomCrop(crop_size),\n",
    "    transforms.RandomHorizontalFlip(), \n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
    "\n",
    "val_transform = transforms.Compose([ \n",
    "    transforms.Resize(crop_size), \n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
    "\n",
    "test_transform = transforms.Compose([ \n",
    "    transforms.Resize(crop_size), \n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
    "\n",
    "class Vocabulary(object):\n",
    "    \"\"\"Simple vocabulary wrapper.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.idx = 0\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if not word in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "\n",
    "    def __call__(self, word):\n",
    "        if not word in self.word2idx:\n",
    "            return self.word2idx['']\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n",
    "    \n",
    "words = [word for word, cnt in counter.items() if cnt >= word_threshold]\n",
    "\n",
    "# Vocabulary 객체 생성\n",
    "vocab = Vocabulary()\n",
    "vocab.add_word('')\n",
    "vocab.add_word('')\n",
    "vocab.add_word('')\n",
    "vocab.add_word('')\n",
    "\n",
    "for word in words:\n",
    "    vocab.add_word(word)\n",
    "\n",
    "# Vocabulary 파일 저장\n",
    "with open(vocab_path, 'wb') as f:\n",
    "    pickle.dump(vocab, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터셋 클래스 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(data.Dataset):\n",
    "    def __init__(self, root, captions, vocab, transform=None):\n",
    "        self.root = root # 이미지가 존재하는 경로\n",
    "        self.captions=captions\n",
    "        self.vocab = vocab\n",
    "        self.transform = transform\n",
    "\n",
    "    # 이미지와 캡션(caption)을 하나씩 꺼내는 메서드\n",
    "    def __getitem__(self, index):\n",
    "        vocab = self.vocab\n",
    "        path = self.captions.loc[index]['image']\n",
    "        caption = self.captions.loc[index]['english']\n",
    "\n",
    "        image = Image.open(self.root+path).convert('RGB')\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # 캡션(caption) 문자열을 토큰 형태로 바꾸기\n",
    "        tokens = nltk.tokenize.word_tokenize(str(caption).lower())\n",
    "        caption = []\n",
    "        caption.append(vocab(''))\n",
    "        caption.extend([vocab(token) for token in tokens])\n",
    "        caption.append(vocab(''))\n",
    "        target = torch.Tensor(caption)\n",
    "        return image, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.captions)\n",
    "    \n",
    "vocab_path = \"./vocab.pkl\" # 전처리된 Vocabulary 파일 경로\n",
    "model_path = \"models/\"\n",
    "# 모델 디렉토리 만들기\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "\n",
    "# Vocabulary 파일 불러오기\n",
    "with open(vocab_path, 'rb') as f:\n",
    "    vocab = pickle.load(f)     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    " #이미지와 캡션(caption)으로 구성된 튜플을 배치(batch)로 만들기\n",
    "def collate_fn(data):\n",
    "    \"\"\"\n",
    "    [입력]\n",
    "    * data: list of tuple (image, caption). \n",
    "        * image: torch tensor of shape (3, 256, 256).\n",
    "        * caption: torch tensor of shape (?); variable length.\n",
    "    [출력]\n",
    "    * images: torch tensor of shape (batch_size, 3, 256, 256).\n",
    "    * targets: torch tensor of shape (batch_size, padded_length).\n",
    "    * lengths: list; valid length for each padded caption.\n",
    "    \"\"\"\n",
    "    # Caption 길이로 각 데이터를 내림차순 정렬\n",
    "    data.sort(key=lambda x: len(x[1]), reverse=True)\n",
    "    images, captions = zip(*data)\n",
    "\n",
    "    # 리스트 형태의 이미지들을 텐서 하나로 합치기(데이터 개수, 3, 256, 256)\n",
    "    images = torch.stack(images, 0)\n",
    "\n",
    "    # 리스트 형태의 캡션들을 텐서 하나로 합치기(데이터 개수, 문장 내 최대 토큰 개수)\n",
    "    lengths = [len(caption) for caption in captions]\n",
    "    targets = torch.zeros(len(captions), max(lengths)).long()\n",
    "    # 하나씩 캡션을 확인하며 앞 부분의 내용을 패딩이 아닌 원래 토큰으로 채우기\n",
    "    for i, cap in enumerate(captions):\n",
    "        end = lengths[i]\n",
    "        targets[i, :end] = cap[:end]\n",
    "    return images, targets, lengths\n",
    "\n",
    "def collate_fn_test(data):\n",
    "    # 기존 순서를 그대로 사용 (차례대로 5개씩 같은 이미지를 표현)\n",
    "    images, captions = zip(*data)\n",
    "\n",
    "    # 리스트 형태의 이미지들을 텐서 하나로 합치기(데이터 개수, 3, 256, 256)\n",
    "    images = torch.stack(images, 0)\n",
    "\n",
    "    # 리스트 형태의 캡션들을 텐서 하나로 합치기(데이터 개수, 문장 내 최대 토큰 개수)\n",
    "    lengths = [len(caption) for caption in captions]\n",
    "    targets = torch.zeros(len(captions), max(lengths)).long()\n",
    "    # 하나씩 캡션을 확인하며 앞 부분의 내용을 패딩이 아닌 원래 토큰으로 채우기\n",
    "    for i, cap in enumerate(captions):\n",
    "        end = lengths[i]\n",
    "        targets[i, :end] = cap[:end]\n",
    "    return images, targets, lengths\n",
    "\n",
    "# 커스텀 Flickr8k 데이터셋을 위한 DataLoader 객체 반환\n",
    "def get_loader(root, captions, vocab, transform, batch_size, shuffle, num_workers, testing):\n",
    "    flickr8k = CustomDataset(root=root, captions=captions, vocab=vocab, transform=transform)\n",
    "    # This will return (images, captions, lengths) for each iteration.\n",
    "    # images: a tensor of shape (batch_size, 3, 224, 224).\n",
    "    # captions: a tensor of shape (batch_size, padded_length).\n",
    "    # lengths: a list indicating valid length for each caption. length is (batch_size).\n",
    "    if not testing:\n",
    "        data_loader = torch.utils.data.DataLoader(dataset=flickr8k, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers, collate_fn=collate_fn)\n",
    "    else:\n",
    "        data_loader = torch.utils.data.DataLoader(dataset=flickr8k, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers, collate_fn=collate_fn_test)\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        # 사전 학습된(pre-trained) ResNet-101을 불러와 FC 레이어를 교체\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        resnet = models.resnet101(pretrained=True)\n",
    "        modules = list(resnet.children())[:-1] # 마지막 FC 레이어를 제거\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        self.linear = nn.Linear(resnet.fc.in_features, embed_size) # 결과(output) 차원을 임베딩 차원으로 변경\n",
    "        self.bn = nn.BatchNorm1d(embed_size, momentum=0.01)\n",
    "\n",
    "    def forward(self, images):\n",
    "        # 입력 이미지에서 특징 벡터(feature vectors)\n",
    "        with torch.no_grad(): # 네트워크의 앞 부분은 변경되지 않도록 하기\n",
    "            features = self.resnet(images)\n",
    "        features = features.reshape(features.size(0), -1)\n",
    "        features = self.bn(self.linear(features))\n",
    "        return features\n",
    "\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers, max_seq_length=20):\n",
    "        # 하이퍼 파라미터(hyper-parameters) 설정 및 레이어 생성\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        self.max_seg_length = max_seq_length\n",
    "\n",
    "    def forward(self, features, captions, lengths):\n",
    "        # 이미지 특징 벡터(feature vectors)로부터 캡션(caption) 생성\n",
    "        embeddings = self.embed(captions)\n",
    "        embeddings = torch.cat((features.unsqueeze(1), embeddings), 1) # 이미지 특징과 임베딩 연결\n",
    "        packed = pack_padded_sequence(embeddings, lengths, batch_first=True) # 패딩을 넣어 차원 맞추기\n",
    "        hiddens, _ = self.lstm(packed) # 다음 hidden state 구하기\n",
    "        outputs = self.linear(hiddens[0])\n",
    "        return outputs\n",
    "\n",
    "    def sample(self, features, states=None):\n",
    "        # 간단히 그리디(greedy) 탐색으로 캡션(caption) 생성하기\n",
    "        sampled_indexes = []\n",
    "        inputs = features.unsqueeze(1)\n",
    "        for i in range(self.max_seg_length):\n",
    "            hiddens, states = self.lstm(inputs, states) # hiddens: (batch_size, 1, hidden_size)\n",
    "            outputs = self.linear(hiddens.squeeze(1)) # outputs: (batch_size, vocab_size)\n",
    "            _, predicted = outputs.max(1) # predicted: (batch_size)\n",
    "            sampled_indexes.append(predicted)\n",
    "            inputs = self.embed(predicted) # inputs: (batch_size, embed_size)\n",
    "            inputs = inputs.unsqueeze(1) # inputs: (batch_size, 1, embed_size)\n",
    "        sampled_indexes = torch.stack(sampled_indexes, 1) # sampled_indexes: (batch_size, max_seq_length)\n",
    "        return sampled_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_workers = 0\n",
    "train_image_path='../../data/dataset/validation/image/'\n",
    "validation_image_path='../../data/dataset/test/image/'\n",
    "train_label=pd.read_csv('../../data/dataset/validation/caption_label.csv',encoding='cp949')\n",
    "validation_label=pd.read_csv('../../data/dataset/test/caption_label.csv',encoding='cp949')\n",
    "# 데이터 로더(data loader) 선언\n",
    "train_data_loader = get_loader(train_image_path, train_label, vocab, train_transform, batch_size, shuffle=True, num_workers=num_workers, testing=False) \n",
    "val_data_loader = get_loader(validation_image_path, validation_label, vocab, val_transform, batch_size, shuffle=False, num_workers=num_workers, testing=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hanok roof1 is on the left side bottom of hanok roof2.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label.loc[0]['english']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gil/anaconda3/envs/LeeYS/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/gil/anaconda3/envs/LeeYS/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# 모델 하이퍼 파라미터 설정\n",
    "embed_size = 256 # 임베딩(embedding) 차원\n",
    "hidden_size = 512 # LSTM hidden states 차원\n",
    "num_layers = 1 # LSTM의 레이어 개수\n",
    "\n",
    "# 모델 객체 선언\n",
    "encoder = EncoderCNN(embed_size).to(device)\n",
    "decoder = DecoderRNN(embed_size, hidden_size, len(vocab), num_layers).to(device)\n",
    "\n",
    "num_epochs = 5\n",
    "learning_rate = 0.001\n",
    "\n",
    "log_step = 20 # 로그를 출력할 스텝(step)\n",
    "save_step = 1000 # 학습된 모델을 저장할 스텝(step)\n",
    "\n",
    "# 손실(loss) 및 최적화 함수 선언\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "params = list(decoder.parameters()) + list(encoder.linear.parameters()) + list(encoder.bn.parameters())\n",
    "optimizer = torch.optim.Adam(params, lr=learning_rate)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Training ]\n",
      "Epoch [0/5], Step [0/109], Elapsed time: 3.2189s\n",
      "Epoch [0/5], Step [20/109], Elapsed time: 25.5353s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/gil/gcubme_ai2/Workspace/YS_Lee/VRD_urban/code/VRD_pytorch/SSG.ipynb 셀 11\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2231302e322e35322e3638222c2275736572223a2267696c227d/home/gil/gcubme_ai2/Workspace/YS_Lee/VRD_urban/code/VRD_pytorch/SSG.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m total_count \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2231302e322e35322e3638222c2275736572223a2267696c227d/home/gil/gcubme_ai2/Workspace/YS_Lee/VRD_urban/code/VRD_pytorch/SSG.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m total_step \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(train_data_loader)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2231302e322e35322e3638222c2275736572223a2267696c227d/home/gil/gcubme_ai2/Workspace/YS_Lee/VRD_urban/code/VRD_pytorch/SSG.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, (images, captions, lengths) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_data_loader):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2231302e322e35322e3638222c2275736572223a2267696c227d/home/gil/gcubme_ai2/Workspace/YS_Lee/VRD_urban/code/VRD_pytorch/SSG.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m     images \u001b[39m=\u001b[39m images\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2231302e322e35322e3638222c2275736572223a2267696c227d/home/gil/gcubme_ai2/Workspace/YS_Lee/VRD_urban/code/VRD_pytorch/SSG.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m     captions \u001b[39m=\u001b[39m captions\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/anaconda3/envs/LeeYS/lib/python3.9/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/LeeYS/lib/python3.9/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/LeeYS/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/LeeYS/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "\u001b[1;32m/home/gil/gcubme_ai2/Workspace/YS_Lee/VRD_urban/code/VRD_pytorch/SSG.ipynb 셀 11\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2231302e322e35322e3638222c2275736572223a2267696c227d/home/gil/gcubme_ai2/Workspace/YS_Lee/VRD_urban/code/VRD_pytorch/SSG.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m vocab \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2231302e322e35322e3638222c2275736572223a2267696c227d/home/gil/gcubme_ai2/Workspace/YS_Lee/VRD_urban/code/VRD_pytorch/SSG.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcaptions\u001b[39m.\u001b[39mloc[index][\u001b[39m'\u001b[39m\u001b[39mimage\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2231302e322e35322e3638222c2275736572223a2267696c227d/home/gil/gcubme_ai2/Workspace/YS_Lee/VRD_urban/code/VRD_pytorch/SSG.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m caption \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcaptions\u001b[39m.\u001b[39;49mloc[index][\u001b[39m'\u001b[39;49m\u001b[39menglish\u001b[39;49m\u001b[39m'\u001b[39;49m]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2231302e322e35322e3638222c2275736572223a2267696c227d/home/gil/gcubme_ai2/Workspace/YS_Lee/VRD_urban/code/VRD_pytorch/SSG.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m image \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mopen(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroot\u001b[39m+\u001b[39mpath)\u001b[39m.\u001b[39mconvert(\u001b[39m'\u001b[39m\u001b[39mRGB\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2231302e322e35322e3638222c2275736572223a2267696c227d/home/gil/gcubme_ai2/Workspace/YS_Lee/VRD_urban/code/VRD_pytorch/SSG.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/LeeYS/lib/python3.9/site-packages/pandas/core/series.py:966\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    961\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_slice\u001b[39m(\u001b[39mself\u001b[39m, slobj: \u001b[39mslice\u001b[39m, axis: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Series:\n\u001b[1;32m    962\u001b[0m     \u001b[39m# axis kwarg is retained for compat with NDFrame method\u001b[39;00m\n\u001b[1;32m    963\u001b[0m     \u001b[39m#  _slice is *always* positional\u001b[39;00m\n\u001b[1;32m    964\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_values(slobj)\n\u001b[0;32m--> 966\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, key):\n\u001b[1;32m    967\u001b[0m     check_deprecated_indexers(key)\n\u001b[1;32m    968\u001b[0m     key \u001b[39m=\u001b[39m com\u001b[39m.\u001b[39mapply_if_callable(key, \u001b[39mself\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "start_time = time.time() # 전체 학습 시간 측정\n",
    "\n",
    "# 모델 학습 진행\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # 먼저 학습 진행하기\n",
    "    print(\"[ Training ]\")\n",
    "    total_loss = 0\n",
    "    total_count = 0\n",
    "    total_step = len(train_data_loader)\n",
    "    for i, (images, captions, lengths) in enumerate(train_data_loader):\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "        targets = pack_padded_sequence(captions, lengths, batch_first=True)[0]\n",
    "\n",
    "        # 순전파(forward), 역전파(backward) 및 학습 진행\n",
    "        features = encoder(images)\n",
    "        outputs = decoder(features, captions, lengths)\n",
    "        loss = criterion(outputs, targets)\n",
    "        decoder.zero_grad()\n",
    "        encoder.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 손실(loss) 값 계산\n",
    "        total_loss += loss.item()\n",
    "        total_count += images.shape[0]\n",
    "        \n",
    "        # 로그(log) 정보 출력\n",
    "        if i % log_step == 0:\n",
    "       \n",
    "            print('Epoch [{}/{}], Step [{}/{}], Elapsed time: {:.4f}s'\n",
    "                  .format(epoch, num_epochs, i, total_step, time.time() - start_time))\n",
    "    print('train time: {:.4f}s'\n",
    "                      .format(time.time() - start_time))\n",
    "    # 모델 파일 저장하기\n",
    "    torch.save(decoder.state_dict(), os.path.join(model_path, f'decoder-{epoch + 1}.ckpt'))\n",
    "    torch.save(encoder.state_dict(), os.path.join(model_path, f'encoder-{epoch + 1}.ckpt'))\n",
    "    print(f\"Model saved: {os.path.join(model_path, f'decoder-{epoch + 1}.ckpt')}\")\n",
    "    print(f\"Model saved: {os.path.join(model_path, f'encoder-{epoch + 1}.ckpt')}\")\n",
    "    start_time = time.time() \n",
    "    # 학습 이후에 평가 진행하기\n",
    "    print(\"[ Validation ]\")\n",
    "    total_loss = 0\n",
    "    total_count = 0\n",
    "    total_step = len(val_data_loader)\n",
    "    with torch.no_grad():\n",
    "        for i, (images, captions, lengths) in enumerate(val_data_loader):\n",
    "            images = images.to(device)\n",
    "            captions = captions.to(device)\n",
    "            targets = pack_padded_sequence(captions, lengths, batch_first=True)[0]\n",
    "\n",
    "            # 순전파(forward) 진행\n",
    "            features = encoder(images)\n",
    "            outputs = decoder(features, captions, lengths)\n",
    "            loss = criterion(outputs, targets)\n",
    "  \n",
    "            # 손실(loss) 값 계산\n",
    "            total_loss += loss.item()\n",
    "            total_count += images.shape[0]\n",
    "\n",
    "            # 로그(log) 정보 출력\n",
    "            if i % log_step == 0:\n",
    "                a=random.randint(5000,10000)\n",
    "                b=random.randint(3000,8000)\n",
    "                print('Epoch [{}/{}], Step [{}/{}], Elapsed time: {:.4f}s'\n",
    "                      .format(epoch, num_epochs, i, total_step, time.time() - start_time))\n",
    "        print('validation time: {:.4f}s'\n",
    "                      .format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Training ]\n",
      "Epoch [0/5], Step [0/109], Elapsed time: 0.4929s\n",
      "Epoch [0/5], Step [20/109], Elapsed time: 9.9024s\n",
      "Epoch [0/5], Step [40/109], Elapsed time: 19.3663s\n",
      "Epoch [0/5], Step [60/109], Elapsed time: 28.8385s\n",
      "Epoch [0/5], Step [80/109], Elapsed time: 38.2898s\n",
      "Epoch [0/5], Step [100/109], Elapsed time: 47.6587s\n",
      "train time: 51.0069s\n",
      "Model saved: models/decoder-1.ckpt\n",
      "Model saved: models/encoder-1.ckpt\n",
      "[ Validation ]\n",
      "Epoch [0/5], Step [0/105], Elapsed time: 0.5744s\n",
      "Epoch [0/5], Step [20/105], Elapsed time: 11.6389s\n",
      "Epoch [0/5], Step [40/105], Elapsed time: 23.3989s\n",
      "Epoch [0/5], Step [60/105], Elapsed time: 34.6737s\n",
      "Epoch [0/5], Step [80/105], Elapsed time: 46.0099s\n",
      "Epoch [0/5], Step [100/105], Elapsed time: 57.2910s\n",
      "validation time: 59.1855s\n",
      "[ Training ]\n",
      "Epoch [1/5], Step [0/109], Elapsed time: 59.6611s\n",
      "Epoch [1/5], Step [20/109], Elapsed time: 69.8269s\n",
      "Epoch [1/5], Step [40/109], Elapsed time: 80.0401s\n",
      "Epoch [1/5], Step [60/109], Elapsed time: 89.8455s\n",
      "Epoch [1/5], Step [80/109], Elapsed time: 99.8396s\n",
      "Epoch [1/5], Step [100/109], Elapsed time: 109.3686s\n",
      "train time: 112.8402s\n",
      "Model saved: models/decoder-2.ckpt\n",
      "Model saved: models/encoder-2.ckpt\n",
      "[ Validation ]\n",
      "Epoch [1/5], Step [0/105], Elapsed time: 0.5355s\n",
      "Epoch [1/5], Step [20/105], Elapsed time: 11.0873s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)\n",
      "\u001b[1;32m/home/gil/gcubme_ai2/Workspace/YS_Lee/VRD_urban/code/VRD_pytorch/Neural_Image_Caption_Generator.ipynb 셀 14\u001b[0m line \u001b[0;36m5\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2231302e322e35322e3638222c2275736572223a2267696c227d/home/gil/gcubme_ai2/Workspace/YS_Lee/VRD_urban/code/VRD_pytorch/Neural_Image_Caption_Generator.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=49'>50</a>\u001b[0m total_step \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(val_data_loader)\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2231302e322e35322e3638222c2275736572223a2267696c227d/home/gil/gcubme_ai2/Workspace/YS_Lee/VRD_urban/code/VRD_pytorch/Neural_Image_Caption_Generator.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=50'>51</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n",
      "\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2231302e322e35322e3638222c2275736572223a2267696c227d/home/gil/gcubme_ai2/Workspace/YS_Lee/VRD_urban/code/VRD_pytorch/Neural_Image_Caption_Generator.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=51'>52</a>\u001b[0m     \u001b[39mfor\u001b[39;00m i, (images, captions, lengths) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(val_data_loader):\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2231302e322e35322e3638222c2275736572223a2267696c227d/home/gil/gcubme_ai2/Workspace/YS_Lee/VRD_urban/code/VRD_pytorch/Neural_Image_Caption_Generator.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=52'>53</a>\u001b[0m         images \u001b[39m=\u001b[39m images\u001b[39m.\u001b[39mto(device)\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2231302e322e35322e3638222c2275736572223a2267696c227d/home/gil/gcubme_ai2/Workspace/YS_Lee/VRD_urban/code/VRD_pytorch/Neural_Image_Caption_Generator.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=53'>54</a>\u001b[0m         captions \u001b[39m=\u001b[39m captions\u001b[39m.\u001b[39mto(device)\n",
      "\n",
      "File \u001b[0;32m~/anaconda3/envs/LeeYS/lib/python3.9/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[1;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n",
      "\u001b[1;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n",
      "\u001b[0;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n",
      "\u001b[1;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[1;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n",
      "\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n",
      "\u001b[1;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "\n",
      "File \u001b[0;32m~/anaconda3/envs/LeeYS/lib/python3.9/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[1;32m    676\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n",
      "\u001b[1;32m    677\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n",
      "\u001b[0;32m--> 678\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n",
      "\u001b[1;32m    679\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n",
      "\u001b[1;32m    680\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "\n",
      "File \u001b[0;32m~/anaconda3/envs/LeeYS/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n",
      "\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n",
      "\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n",
      "\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "\n",
      "File \u001b[0;32m~/anaconda3/envs/LeeYS/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n",
      "\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n",
      "\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n",
      "\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "\n",
      "\u001b[1;32m/home/gil/gcubme_ai2/Workspace/YS_Lee/VRD_urban/code/VRD_pytorch/Neural_Image_Caption_Generator.ipynb 셀 14\u001b[0m line \u001b[0;36m1\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2231302e322e35322e3638222c2275736572223a2267696c227d/home/gil/gcubme_ai2/Workspace/YS_Lee/VRD_urban/code/VRD_pytorch/Neural_Image_Caption_Generator.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcaptions\u001b[39m.\u001b[39mloc[index][\u001b[39m'\u001b[39m\u001b[39mimage\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2231302e322e35322e3638222c2275736572223a2267696c227d/home/gil/gcubme_ai2/Workspace/YS_Lee/VRD_urban/code/VRD_pytorch/Neural_Image_Caption_Generator.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m caption \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcaptions\u001b[39m.\u001b[39mloc[index][\u001b[39m'\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2231302e322e35322e3638222c2275736572223a2267696c227d/home/gil/gcubme_ai2/Workspace/YS_Lee/VRD_urban/code/VRD_pytorch/Neural_Image_Caption_Generator.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m image \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39;49mopen(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mroot\u001b[39m+\u001b[39;49mpath)\u001b[39m.\u001b[39mconvert(\u001b[39m'\u001b[39m\u001b[39mRGB\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2231302e322e35322e3638222c2275736572223a2267696c227d/home/gil/gcubme_ai2/Workspace/YS_Lee/VRD_urban/code/VRD_pytorch/Neural_Image_Caption_Generator.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2231302e322e35322e3638222c2275736572223a2267696c227d/home/gil/gcubme_ai2/Workspace/YS_Lee/VRD_urban/code/VRD_pytorch/Neural_Image_Caption_Generator.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m     image \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform(image)\n",
      "\n",
      "File \u001b[0;32m~/anaconda3/envs/LeeYS/lib/python3.9/site-packages/PIL/Image.py:3227\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n",
      "\u001b[1;32m   3224\u001b[0m     filename \u001b[39m=\u001b[39m fp\n",
      "\u001b[1;32m   3226\u001b[0m \u001b[39mif\u001b[39;00m filename:\n",
      "\u001b[0;32m-> 3227\u001b[0m     fp \u001b[39m=\u001b[39m builtins\u001b[39m.\u001b[39;49mopen(filename, \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "\u001b[1;32m   3228\u001b[0m     exclusive_fp \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[1;32m   3230\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "start_time = time.time() # 전체 학습 시간 측정\n",
    "\n",
    "# 모델 학습 진행\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # 먼저 학습 진행하기\n",
    "    print(\"[ Training ]\")\n",
    "    total_loss = 0\n",
    "    total_count = 0\n",
    "    total_step = len(train_data_loader)\n",
    "    for i, (images, captions, lengths) in enumerate(train_data_loader):\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "        targets = pack_padded_sequence(captions, lengths, batch_first=True)[0]\n",
    "\n",
    "        # 순전파(forward), 역전파(backward) 및 학습 진행\n",
    "        features = encoder(images)\n",
    "        outputs = decoder(features, captions, lengths)\n",
    "        loss = criterion(outputs, targets)\n",
    "        decoder.zero_grad()\n",
    "        encoder.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 손실(loss) 값 계산\n",
    "        total_loss += loss.item()\n",
    "        total_count += images.shape[0]\n",
    "        \n",
    "        # 로그(log) 정보 출력\n",
    "        if i % log_step == 0:\n",
    "       \n",
    "            print('Epoch [{}/{}], Step [{}/{}], Elapsed time: {:.4f}s'\n",
    "                  .format(epoch, num_epochs, i, total_step, time.time() - start_time))\n",
    "    print('train time: {:.4f}s'\n",
    "                      .format(time.time() - start_time))\n",
    "    # 모델 파일 저장하기\n",
    "    torch.save(decoder.state_dict(), os.path.join(model_path, f'decoder-{epoch + 1}.ckpt'))\n",
    "    torch.save(encoder.state_dict(), os.path.join(model_path, f'encoder-{epoch + 1}.ckpt'))\n",
    "    print(f\"Model saved: {os.path.join(model_path, f'decoder-{epoch + 1}.ckpt')}\")\n",
    "    print(f\"Model saved: {os.path.join(model_path, f'encoder-{epoch + 1}.ckpt')}\")\n",
    "    start_time = time.time() \n",
    "    # 학습 이후에 평가 진행하기\n",
    "    print(\"[ Validation ]\")\n",
    "    total_loss = 0\n",
    "    total_count = 0\n",
    "    total_step = len(val_data_loader)\n",
    "    with torch.no_grad():\n",
    "        for i, (images, captions, lengths) in enumerate(val_data_loader):\n",
    "            images = images.to(device)\n",
    "            captions = captions.to(device)\n",
    "            targets = pack_padded_sequence(captions, lengths, batch_first=True)[0]\n",
    "\n",
    "            # 순전파(forward) 진행\n",
    "            features = encoder(images)\n",
    "            outputs = decoder(features, captions, lengths)\n",
    "            loss = criterion(outputs, targets)\n",
    "  \n",
    "            # 손실(loss) 값 계산\n",
    "            total_loss += loss.item()\n",
    "            total_count += images.shape[0]\n",
    "\n",
    "            # 로그(log) 정보 출력\n",
    "            if i % log_step == 0:\n",
    "                a=random.randint(5000,10000)\n",
    "                b=random.randint(3000,8000)\n",
    "                print('Epoch [{}/{}], Step [{}/{}], Elapsed time: {:.4f}s'\n",
    "                      .format(epoch, num_epochs, i, total_step, time.time() - start_time))\n",
    "        print('validation time: {:.4f}s'\n",
    "                      .format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측한 문장의 수: 2677\n",
      "정답 문장 집합의 수 (5개씩): 2677\n"
     ]
    }
   ],
   "source": [
    "print(\"예측한 문장의 수:\", len(predictions))\n",
    "print(\"정답 문장 집합의 수 (5개씩):\", len(answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 정답 캡션들 ]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[ 예측된 캡션 ]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "index = 2\n",
    "print(\"[ 정답 캡션들 ]\")\n",
    "for answer in answers[index]:\n",
    "    print(answer)\n",
    "\n",
    "print(\"[ 예측된 캡션 ]\")\n",
    "print(predictions[index])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LeeYS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
