{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np \n",
    "import os\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "from tqdm.auto import tqdm\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import torch.utils.data as data\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "import pickle\n",
    "from torchvision import transforms\n",
    "from tqdm.auto import tqdm\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/coco/annotations_trainval2017/annotations/captions_val2017.json') as f:\n",
    "    val_json_object = json.load(f)\n",
    "with open('../../data/coco/annotations_trainval2014/annotations/captions_train2014.json') as f:\n",
    "    train_json_object = json.load(f)\n",
    "\n",
    "train_annotation=train_json_object['annotations']\n",
    "train_image=train_json_object['images']\n",
    "test_annotation=val_json_object['annotations']\n",
    "test_image=val_json_object['images']\n",
    "train_caption_path = \"../../data/coco/pre_train/captions.txt\" # 크기가 조정된 이미지의 캡션(caption)이 담길 경로 (학습)\n",
    "test_caption_path = \"../../data/coco/pre_test/captions.txt\" # 크기가 조정된 이미지의 캡션(caption)이 담길 경로 (평가)\n",
    "\n",
    "class Vocabulary(object):\n",
    "    \"\"\"Simple vocabulary wrapper.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.idx = 0\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if not word in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "\n",
    "    def __call__(self, word):\n",
    "        if not word in self.word2idx:\n",
    "            return self.word2idx['']\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400224"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_annotation.extend(test_annotation)\n",
    "len(test_annotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#image_preprocessing\n",
    "\n",
    "train_image_list = glob(\"../../data/coco/train2014/train2014/*.jpg\") # 원본 이미지 파일 경로\n",
    "test_image_list = glob('../../data/coco/val2017/val2017/*.jpg') # 원본 이미지 파일 경로\n",
    "train_dst_image_list = [f.replace('/train2014/train2014', '/pre_train') for f in train_image_list]\n",
    "test_dst_image_list = [f.replace('/val2017/val2017', '/pre_test') for f in test_image_list]\n",
    "size = [256, 256] # 조정될 이미지 크기\n",
    "\n",
    "\n",
    "def resize_image(image, size):\n",
    "    # 이미지를 특정 크기로 조정\n",
    "    return image.resize(size, Image.ANTIALIAS)\n",
    "\n",
    "for i in tqdm(range(len(train_image_list))):\n",
    "    img=Image.open(train_image_list[i])\n",
    "    img = resize_image(img, size)\n",
    "    img.save(train_dst_image_list[i])\n",
    "\n",
    "for i in tqdm(range(len(test_image_list))):\n",
    "    img=Image.open(test_image_list[i])\n",
    "    img = resize_image(img, size)\n",
    "    img.save(test_dst_image_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Capting preprocessing\n",
    "vocab_path = \"./vocab.pkl\" # 단어 사전 결과 파일\n",
    "word_threshold = 4 # 최소 단어 등장 횟수\n",
    "\n",
    "\n",
    "\n",
    "counter = Counter()\n",
    "\n",
    "\n",
    "for i in tqdm(range(len(train_image))):\n",
    "    file_name=train_image[i]['file_name']\n",
    "    file_id=train_image[i]['id']\n",
    "    list_search = [ item for item in train_annotation if item['image_id'] == file_id]\n",
    "    for j in range(len(list_search)):\n",
    "        \n",
    "        caption = list_search[j]['caption'] # 캡션(caption) 문자열 기록\n",
    "        tokens = nltk.tokenize.word_tokenize(caption.lower()) # 문자열 토큰화\n",
    "        counter.update(tokens) # 각 토큰의 개수 세기\n",
    "        \n",
    "\n",
    "for i in tqdm(range(len(test_image))):\n",
    "    file_name=test_image[i]['file_name']\n",
    "    file_id=test_image[i]['id']\n",
    "    list_search = [ item for item in test_annotation if item['image_id'] == file_id]\n",
    "    for j in range(len(list_search)):\n",
    "        caption = list_search[j]['caption']\n",
    "        tokens = nltk.tokenize.word_tokenize(caption.lower()) # 문자열 토큰화\n",
    "        counter.update(tokens) # 각 토큰의 개수 세기\n",
    "        \n",
    "words = [word for word, cnt in counter.items() if cnt >= word_threshold]\n",
    "# Vocabulary 객체 생성\n",
    "vocab = Vocabulary()\n",
    "vocab.add_word('')\n",
    "vocab.add_word('')\n",
    "vocab.add_word('')\n",
    "vocab.add_word('') # unknown 토큰\n",
    "words\n",
    "for word in words:\n",
    "    vocab.add_word(word)\n",
    "with open(vocab_path, 'wb') as f:\n",
    "    pickle.dump(vocab, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_annotation[4]['image_id']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class cocoDataset(data.Dataset):\n",
    "    def __init__(self, root, captions,class_1, vocab, transform=None):\n",
    "        self.root = root # 이미지가 존재하는 경로\n",
    "        \n",
    "        self.captions = [] # 캡션(caption) 정보를 담을 리스트\n",
    "        for line in captions: # 첫 번째 줄부터 바로 캡션 정보 존재\n",
    "            caption = line['caption'] \n",
    "            if class_1=='train':\n",
    "               path= 'COCO_train2014_'+str(line['image_id']).zfill(12)+'.jpg'\n",
    "            else:\n",
    "               path= str(line['image_id']).zfill(12)+'.jpg'\n",
    "            self.captions.append((path, caption))\n",
    "        self.vocab = vocab\n",
    "        self.transform = transform\n",
    "\n",
    "    # 이미지와 캡션(caption)을 하나씩 꺼내는 메서드\n",
    "    def __getitem__(self, index):\n",
    "        vocab = self.vocab\n",
    "        path = self.captions[index][0]\n",
    "        caption = self.captions[index][1]\n",
    "\n",
    "        image = Image.open(os.path.join(self.root, path)).convert('RGB')\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # 캡션(caption) 문자열을 토큰 형태로 바꾸기\n",
    "        tokens = nltk.tokenize.word_tokenize(str(caption).lower())\n",
    "        caption = []\n",
    "        caption.append(vocab(''))\n",
    "        caption.extend([vocab(token) for token in tokens])\n",
    "        caption.append(vocab(''))\n",
    "        target = torch.Tensor(caption)\n",
    "        return image, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.captions)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지와 캡션(caption)으로 구성된 튜플을 배치(batch)로 만들기\n",
    "def collate_fn(data):\n",
    "    \"\"\"\n",
    "    [입력]\n",
    "    * data: list of tuple (image, caption). \n",
    "        * image: torch tensor of shape (3, 256, 256).\n",
    "        * caption: torch tensor of shape (?); variable length.\n",
    "    [출력]\n",
    "    * images: torch tensor of shape (batch_size, 3, 256, 256).\n",
    "    * targets: torch tensor of shape (batch_size, padded_length).\n",
    "    * lengths: list; valid length for each padded caption.\n",
    "    \"\"\"\n",
    "    # Caption 길이로 각 데이터를 내림차순 정렬\n",
    "    data.sort(key=lambda x: len(x[1]), reverse=True)\n",
    "    images, captions = zip(*data)\n",
    "\n",
    "    # 리스트 형태의 이미지들을 텐서 하나로 합치기(데이터 개수, 3, 256, 256)\n",
    "    images = torch.stack(images, 0)\n",
    "\n",
    "    # 리스트 형태의 캡션들을 텐서 하나로 합치기(데이터 개수, 문장 내 최대 토큰 개수)\n",
    "    lengths = [len(caption) for caption in captions]\n",
    "    targets = torch.zeros(len(captions), max(lengths)).long()\n",
    "    # 하나씩 캡션을 확인하며 앞 부분의 내용을 패딩이 아닌 원래 토큰으로 채우기\n",
    "    for i, cap in enumerate(captions):\n",
    "        end = lengths[i]\n",
    "        targets[i, :end] = cap[:end]\n",
    "    return images, targets, lengths\n",
    "\n",
    "def collate_fn_test(data):\n",
    "    # 기존 순서를 그대로 사용 (차례대로 5개씩 같은 이미지를 표현)\n",
    "    images, captions = zip(*data)\n",
    "\n",
    "    # 리스트 형태의 이미지들을 텐서 하나로 합치기(데이터 개수, 3, 256, 256)\n",
    "    images = torch.stack(images, 0)\n",
    "\n",
    "    # 리스트 형태의 캡션들을 텐서 하나로 합치기(데이터 개수, 문장 내 최대 토큰 개수)\n",
    "    lengths = [len(caption) for caption in captions]\n",
    "    targets = torch.zeros(len(captions), max(lengths)).long()\n",
    "    # 하나씩 캡션을 확인하며 앞 부분의 내용을 패딩이 아닌 원래 토큰으로 채우기\n",
    "    for i, cap in enumerate(captions):\n",
    "        end = lengths[i]\n",
    "        targets[i, :end] = cap[:end]\n",
    "    return images, targets, lengths\n",
    "\n",
    "# 커스텀 Flickr8k 데이터셋을 위한 DataLoader 객체 반환\n",
    "def get_loader(root, captions,image, vocab, transform, batch_size, shuffle, num_workers, testing):\n",
    "    coco = cocoDataset(root=root, captions=captions,class_1=image, vocab=vocab, transform=transform)\n",
    "    # This will return (images, captions, lengths) for each iteration.\n",
    "    # images: a tensor of shape (batch_size, 3, 224, 224).\n",
    "    # captions: a tensor of shape (batch_size, padded_length).\n",
    "    # lengths: a list indicating valid length for each caption. length is (batch_size).\n",
    "    if not testing:\n",
    "        data_loader = torch.utils.data.DataLoader(dataset=coco, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers, collate_fn=collate_fn)\n",
    "    else:\n",
    "        data_loader = torch.utils.data.DataLoader(dataset=coco, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers, collate_fn=collate_fn_test)\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        # 사전 학습된(pre-trained) ResNet-152을 불러와 FC 레이어를 교체\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        resnet = models.resnet152(pretrained=True)\n",
    "        modules = list(resnet.children())[:-1] # 마지막 FC 레이어를 제거\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        self.linear = nn.Linear(resnet.fc.in_features, embed_size) # 결과(output) 차원을 임베딩 차원으로 변경\n",
    "        self.bn = nn.BatchNorm1d(embed_size, momentum=0.01)\n",
    "\n",
    "    def forward(self, images):\n",
    "        # 입력 이미지에서 특징 벡터(feature vectors)\n",
    "        with torch.no_grad(): # 네트워크의 앞 부분은 변경되지 않도록 하기\n",
    "            features = self.resnet(images)\n",
    "        features = features.reshape(features.size(0), -1)\n",
    "        features = self.bn(self.linear(features))\n",
    "        return features\n",
    "\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers, max_seq_length=20):\n",
    "        # 하이퍼 파라미터(hyper-parameters) 설정 및 레이어 생성\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        self.max_seg_length = max_seq_length\n",
    "\n",
    "    def forward(self, features, captions, lengths):\n",
    "        # 이미지 특징 벡터(feature vectors)로부터 캡션(caption) 생성\n",
    "        embeddings = self.embed(captions)\n",
    "        embeddings = torch.cat((features.unsqueeze(1), embeddings), 1) # 이미지 특징과 임베딩 연결\n",
    "        packed = pack_padded_sequence(embeddings, lengths, batch_first=True) # 패딩을 넣어 차원 맞추기\n",
    "        hiddens, _ = self.lstm(packed) # 다음 hidden state 구하기\n",
    "        outputs = self.linear(hiddens[0])\n",
    "        return outputs\n",
    "\n",
    "    def sample(self, features, states=None):\n",
    "        # 간단히 그리디(greedy) 탐색으로 캡션(caption) 생성하기\n",
    "        sampled_indexes = []\n",
    "        inputs = features.unsqueeze(1)\n",
    "        for i in range(self.max_seg_length):\n",
    "            hiddens, states = self.lstm(inputs, states) # hiddens: (batch_size, 1, hidden_size)\n",
    "            outputs = self.linear(hiddens.squeeze(1)) # outputs: (batch_size, vocab_size)\n",
    "            _, predicted = outputs.max(1) # predicted: (batch_size)\n",
    "            sampled_indexes.append(predicted)\n",
    "            inputs = self.embed(predicted) # inputs: (batch_size, embed_size)\n",
    "            inputs = inputs.unsqueeze(1) # inputs: (batch_size, 1, embed_size)\n",
    "        sampled_indexes = torch.stack(sampled_indexes, 1) # sampled_indexes: (batch_size, max_seq_length)\n",
    "        return sampled_indexes\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prepro import *\n",
    "model_path ='../../model/' # 학습된 모델이 저장될 경로\n",
    "crop_size = 224 # 랜덤하게 잘라낼 이미지 크기\n",
    "vocab_path = \"./vocab.pkl\" # 전처리된 Vocabulary 파일 경로\n",
    "vocab = Vocabulary()\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "# Vocabulary 파일 불러오기\n",
    "with open(vocab_path, 'rb') as f:\n",
    "    vocab = pickle.load(f)\n",
    "    \n",
    "# 사전 학습된(pre-trained) ResNet에 적용된 전처리 및 정규화 파라미터를 그대로 사용합니다.\n",
    "train_transform = transforms.Compose([ \n",
    "    transforms.RandomCrop(crop_size),\n",
    "    transforms.RandomHorizontalFlip(), \n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
    "\n",
    "val_transform = transforms.Compose([ \n",
    "    transforms.Resize(crop_size), \n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
    "\n",
    "test_transform = transforms.Compose([ \n",
    "    transforms.Resize(crop_size), \n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
    "\n",
    "batch_size = 128\n",
    "num_workers = 2\n",
    "\n",
    "train_data_loader = get_loader('../../data/coco/pre_train', train_annotation,'train', vocab, train_transform, batch_size, shuffle=True, num_workers=num_workers, testing=False) \n",
    "test_data_loader = get_loader('../../data/coco/pre_test', test_annotation,'test', vocab, test_transform, batch_size, shuffle=False, num_workers=num_workers, testing=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 모델 하이퍼 파라미터 설정\n",
    "embed_size = 256 # 임베딩(embedding) 차원\n",
    "hidden_size = 512 # LSTM hidden states 차원\n",
    "num_layers = 1 # LSTM의 레이어 개수\n",
    "\n",
    "# 모델 객체 선언\n",
    "encoder = EncoderCNN(embed_size).to(device)\n",
    "decoder = DecoderRNN(embed_size, hidden_size, len(vocab), num_layers).to(device)\n",
    "\n",
    "num_epochs = 5\n",
    "learning_rate = 0.001\n",
    "\n",
    "log_step = 3# 로그를 출력할 스텝(step)\n",
    "save_step = 1000 # 학습된 모델을 저장할 스텝(step)\n",
    "\n",
    "# 손실(loss) 및 최적화 함수 선언\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "params = list(decoder.parameters()) + list(encoder.linear.parameters()) + list(encoder.bn.parameters())\n",
    "optimizer = torch.optim.Adam(params, lr=learning_rate)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "start_time = time.time() # 전체 학습 시간 측정\n",
    "\n",
    "# 모델 학습 진행\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # 먼저 학습 진행하기\n",
    "    print(\"[ Training ]\")\n",
    "    total_loss = 0\n",
    "    total_count = 0\n",
    "    total_step = len(train_data_loader)\n",
    "    train_data_loder_tqdm=tqdm(train_data_loader)\n",
    "    for i, (images, captions, lengths) in enumerate(train_data_loder_tqdm):\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "        targets = pack_padded_sequence(captions, lengths, batch_first=True)[0]\n",
    "\n",
    "        # 순전파(forward), 역전파(backward) 및 학습 진행\n",
    "        features = encoder(images)\n",
    "        outputs = decoder(features, captions, lengths)\n",
    "        loss = criterion(outputs, targets)\n",
    "        decoder.zero_grad()\n",
    "        encoder.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 손실(loss) 값 계산\n",
    "        total_loss += loss.item()\n",
    "        total_count += images.shape[0]\n",
    "\n",
    "        # 로그(log) 정보 출력\n",
    "        train_data_loder_tqdm.set_description(f'Epoch [{epoch}/{num_epochs}], Step [{i}/{total_step}], Average Loss: {(total_loss / total_count):.4f}, Perplexity: {np.exp(loss.item()):5.4f}, Elapsed time: {( time.time() - start_time):.4f}s',)\n",
    "    \n",
    "\n",
    "    # 모델 파일 저장하기\n",
    "    torch.save(decoder.state_dict(), os.path.join(model_path, f'decoder-{epoch + 1}.ckpt'))\n",
    "    torch.save(encoder.state_dict(), os.path.join(model_path, f'encoder-{epoch + 1}.ckpt'))\n",
    "    print(f\"Model saved: {os.path.join(model_path, f'decoder-{epoch + 1}.ckpt')}\")\n",
    "    print(f\"Model saved: {os.path.join(model_path, f'encoder-{epoch + 1}.ckpt')}\")\n",
    "\n",
    "    # 학습 이후에 평가 진행하기\n",
    "    print(\"[ Validation ]\")\n",
    "    total_loss = 0\n",
    "    total_count = 0\n",
    "    total_step = len(test_data_loader)\n",
    "    test_data_loder_tqdm=tqdm(test_data_loader)\n",
    "    with torch.no_grad():\n",
    "        for i, (images, captions, lengths) in enumerate(test_data_loder_tqdm):\n",
    "            images = images.to(device)\n",
    "            captions = captions.to(device)\n",
    "            targets = pack_padded_sequence(captions, lengths, batch_first=True)[0]\n",
    "\n",
    "            # 순전파(forward) 진행\n",
    "            features = encoder(images)\n",
    "            outputs = decoder(features, captions, lengths)\n",
    "            loss = criterion(outputs, targets)\n",
    "  \n",
    "            # 손실(loss) 값 계산\n",
    "            total_loss += loss.item()\n",
    "            total_count += images.shape[0]\n",
    "            test_data_loder_tqdm.set_description(f'test_Epoch [{epoch}/{num_epochs}], Step [{i}/{total_step}], Average Loss: {(total_loss / total_count):.4f}, Perplexity: {np.exp(loss.item()):5.4f}, Elapsed time: {( time.time() - start_time):.4f}s',)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path, transform=None):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image = image.resize([224, 224], Image.LANCZOS)\n",
    "\n",
    "    if transform is not None:\n",
    "        image = transform(image).unsqueeze(0)\n",
    "    \n",
    "    return image\n",
    "\n",
    "image_path = \"../../data/coco/pre_test/000000000285.jpg\" # 캡션(caption)을 생성할 입력 이미지\n",
    "encoder_path = \"../../model/encoder-5.ckpt\" # path for trained encoder\n",
    "decoder_path = \"../../model/decoder-5.ckpt\" # path for trained decoder\n",
    "vocab_path = \"./vocab.pkl\" # path for vocabulary wrapper\n",
    "\n",
    "# Model parameters (should be same as paramters in train.py)\n",
    "embed_size = 256 # dimension of word embedding vectors\n",
    "hidden_size = 512 # dimension of lstm hidden states\n",
    "num_layers = 1 # number of layers in lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02f9113fa3104f45968a2a4c421ba9d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "predictions = []\n",
    "answers = []\n",
    "answers_per_image = []\n",
    "encoder = EncoderCNN(embed_size).eval() # eval mode (batchnorm uses moving mean/variance)\n",
    "decoder = DecoderRNN(embed_size, hidden_size, len(vocab), num_layers)\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)\n",
    "\n",
    "# Load the trained model parameters\n",
    "encoder.load_state_dict(torch.load(encoder_path))\n",
    "decoder.load_state_dict(torch.load(decoder_path))\n",
    "total_step = len(test_data_loader)\n",
    "cnt = 0\n",
    "test_data_loder_tqdm=tqdm(test_data_loader)\n",
    "with torch.no_grad():\n",
    "    for i, (images, captions, lengths) in enumerate(test_data_loder_tqdm):\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "\n",
    "        # 순전파(forward) 진행\n",
    "        features = encoder(images)\n",
    "        sampled_ids_list = decoder.sample(features)\n",
    "\n",
    "        for index in range(len(images)):\n",
    "            sampled_ids = sampled_ids_list[index].cpu().numpy()\n",
    "\n",
    "            # 정답 문장(answer sentences)\n",
    "            answer = []\n",
    "            for word_id in captions[index]: # 하나씩 단어 인덱스를 확인하며\n",
    "                word = vocab.idx2word[word_id.item()] # 단어 문자열로 바꾸어 삽입\n",
    "                answer.append(word)\n",
    "                \n",
    "            answers_per_image.append(answer[1:-1]) # 정답 문장을 삽입 (과 는 제외)\n",
    "\n",
    "            \n",
    "            if (cnt + 1) % 5 == 0: # 이미지당 캡션이 5개씩 존재\n",
    "                answers.append(answers_per_image) # 5개를 한꺼번에 리스트로 삽입\n",
    "                answers_per_image = []\n",
    "\n",
    "                # 예측한 문장(predicted sentences)\n",
    "                prediction = []\n",
    "                for word_id in sampled_ids: # 하나씩 단어 인덱스를 확인하며\n",
    "                    word = vocab.idx2word[word_id] # 단어 문자열로 바꾸어 삽입\n",
    "                    prediction.append(word)\n",
    "                \n",
    "                predictions.append(prediction[1:-1]) # 예측한 문장에 대해서는 1개만 삽입 (과 는 제외\n",
    "            cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torchtext.data.metrics import bleu_score\n",
    "\n",
    "\n",
    "individual_bleu1_score = bleu_score(predictions, answers, max_n=4, weights=[1, 0, 0, 0])\n",
    "\n",
    "\n",
    "print(f'BLEU1 score = {individual_bleu1_score * 100:.2f}') \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions[2][2].item()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LeeYS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
